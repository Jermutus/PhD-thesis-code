#This code was used to get tf-idf for term 'AI' ('KI' for German articles) to select subset of artciles from chapter 6 for manual analysis as reported in chapter 7 of the thesis
# the code was taken and adapted from  https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf
#code was tweaked for each outlet and for German articles (see notes)
#code was run for each outlet separately (i.e. 8 times - one time for each folder/outlet) to get tf-idf of 'AI'/'KI' for each outlet so could select articles for each outlet that focused most on AI

from pathlib import Path

#Path ="/Users/.../... "


all_txt_files =[]

for file in Path("FAZ").rglob("*.txt"):        #specify folder here e.g. 'FAZ' for folder of Frankfurter Allgemeine Zeitung
     all_txt_files.append(file.parent / file.name)
# counts the length of the list
n_files = len(all_txt_files)
print(n_files)

all_docs = []
for txt_file in all_txt_files:
    with open(txt_file, encoding='cp1252') as f: # add encoding='cp1252' when running for German articles and change setting to windows-1252; for English artciles did not specify encoding
        txt_file_as_string = f.read()
    all_docs.append(txt_file_as_string)


#import the TfidfVectorizer from Scikit-Learn.
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words=None, use_idf=True, norm=None)
transformed_documents = vectorizer.fit_transform(all_docs)

transformed_documents_as_array = transformed_documents.toarray()
# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list
len(transformed_documents_as_array)

import pandas as pd

# make the output folder if it doesn't already exist
Path("./tf_idf_output").mkdir(parents=True, exist_ok=True)

# construct a list of output file paths using the previous list of text files the relative path for tf_idf_output
output_filenames = [str(txt_file).replace(".txt", ".csv").replace("txt/", "tf_idf_output/") for txt_file in all_txt_files]

# loop each item in transformed_documents_as_array, using enumerate to keep track of the current position
for counter, doc in enumerate(transformed_documents_as_array):
    # construct a dataframe
    tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), doc))
    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)

    # output to a csv using the enumerated value for the filename
    one_doc_as_df.to_csv(output_filenames[counter])


